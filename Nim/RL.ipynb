{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d1dc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e67ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.01 #alpha\n",
    "decay_gamma=0.7 #gamma\n",
    "exp_rate=0.3 #epsilon\n",
    "starting_player='agent'\n",
    "list_of_winners=[]\n",
    "policy={}\n",
    "initial_board=[3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b021381",
   "metadata": {
    "code_folding": [
     1,
     12,
     18,
     31,
     39,
     43,
     54,
     65,
     73,
     94,
     99,
     114,
     119,
     145,
     173,
     193
    ]
   },
   "outputs": [],
   "source": [
    "class NimGame:\n",
    "    def __init__(self, init_board):\n",
    "        self.board=initial_board.copy()\n",
    "        self.number_of_piles=np.count_nonzero(self.board)\n",
    "        self.player=starting_player\n",
    "        self.winner=None\n",
    "        self.starting_player=starting_player\n",
    "        self.states_value={}\n",
    "        self.states=[]\n",
    "        self.exp_rate=exp_rate\n",
    "        self.lr=learning_rate\n",
    "        self.gamma=decay_gamma\n",
    "    def change_player(self):\n",
    "        \"\"\"A function for changing player\"\"\"\n",
    "        if self.player=='agent':\n",
    "            self.player='dummy'\n",
    "        else:\n",
    "            self.player='agent'\n",
    "    def get_reward(self):\n",
    "        \"\"\"This function determines the reward to give.\n",
    "            Returns the reward.\"\"\"\n",
    "        if self.player =='agent':\n",
    "            if sum(self.board)==1:\n",
    "                return 1 \n",
    "        if self.is_game_over():\n",
    "            if self.player == 'agent': #if player 1 takes last stick\n",
    "                return -1\n",
    "            else:\n",
    "                return 1 # in player 2 takes last stick\n",
    "        else:\n",
    "            return 0 #if game is not over\n",
    "    def get_legal_actions(self):\n",
    "        \"\"\"This function determines all legal actions on the current board state.\n",
    "            Returns a list of legal actions.\"\"\"\n",
    "        actions = []\n",
    "        for i in np.nonzero(self.board)[0]:#indices of non-zero piles\n",
    "            for j in range(1, self.board[i]+1): #number of stones\n",
    "                actions.append((i, j))\n",
    "        return actions\n",
    "    def is_game_over(self):\n",
    "        \"\"\"A function for determining whether the game is over.\n",
    "            Returns True if the game is over and False otherwise\"\"\"\n",
    "        return sum(self.board)==0\n",
    "    def possible_next_states(self, state):\n",
    "        \"\"\"Function to consider all possible board states which is obtainable from a given state.\n",
    "            Input is a board state for which you want to find all possible obtainable states from.\n",
    "            Returns a list of all possible obtainable board states.\"\"\"\n",
    "        list_to_return=[]\n",
    "        for i in range(state[0]+1):\n",
    "            for j in range(state[1]+1):\n",
    "                list_to_return.append([state[0]-i, state[1]-j])\n",
    "        if state in list_to_return:\n",
    "            list_to_return.remove(state)\n",
    "        return list_to_return\n",
    "    def feedReward(self, reward):\n",
    "        \"\"\"Function for updating the Q-table\"\"\"\n",
    "        for st in reversed(self.states):  # goes through all saved board states of this game\n",
    "            possible_next_st=self.possible_next_states(st)\n",
    "            temp=-1000\n",
    "            for i in possible_next_st:\n",
    "                if self.states_value.get(str(i)) is not None and self.states_value.get(str(i))>temp:\n",
    "                    temp=self.states_value.get(str(i))\n",
    "            if self.states_value.get(str(st)) is None: \n",
    "                self.states_value[str(st)] = 0  # initialise a value for the state\n",
    "            self.states_value[str(st)] += self.lr * (reward -self.gamma*temp- self.states_value[str(st)])\n",
    "    def reset(self,init_state):\n",
    "        \"\"\"A function for resetting the game after an ended game.\"\"\"\n",
    "        self.board=init_state\n",
    "        self.states=[]\n",
    "        self.number_of_piles=np.count_nonzero(self.board)\n",
    "        self.player=self.starting_player\n",
    "        self.winner=None\n",
    "        return\n",
    "    def nim_sum(self, numbers):\n",
    "        \"\"\"Function for calculating the nim sum.\n",
    "            Input is a list of numbers.\n",
    "            Returns Nim sum of the numbers in the list.\"\"\"\n",
    "        binary_numbers = [bin(num)[2:].zfill(len(bin(max(numbers))) - 2) for num in numbers]\n",
    "        column_sums = [sum(int(binary[i]) for binary in binary_numbers) for i in range(len(binary_numbers[0]))]\n",
    "        nim_sum = ''.join(['0' if sum % 2 == 0 else '1' for sum in column_sums])\n",
    "        return int(nim_sum, 2)\n",
    "    def choose_optimal_action(self):\n",
    "        \"\"\"This function returns the optimal action, i.e. such that the nim sum is zero.\n",
    "        if this is not possible, then it returns a random action\"\"\"\n",
    "        if self.nim_sum(self.board)==0:\n",
    "            return self.choose_random_action()\n",
    "        else:\n",
    "            actions=self.get_legal_actions()\n",
    "            for a in actions:\n",
    "                test_board=self.board.copy()\n",
    "                pile_number, amount_to_remove=a\n",
    "                test_board[pile_number]-=amount_to_remove\n",
    "                if self.nim_sum(test_board)==0:\n",
    "                    return a\n",
    "    def choose_random_action(self):\n",
    "        \"\"\"A function for choosing an action randomly.\n",
    "            Returns an action.\"\"\"\n",
    "        action=self.get_legal_actions()\n",
    "        return random.choice(action)\n",
    "    def choose_smart_action(self):\n",
    "        \"\"\"A function for choosing an action according to the Q-table.\n",
    "        Returns an action.\"\"\"\n",
    "        actions=self.get_legal_actions()\n",
    "        value_max = -999\n",
    "        for p in actions:\n",
    "            board_copy=self.board.copy()\n",
    "            pile_number, amount_to_remove=p\n",
    "            board_copy[pile_number]-=amount_to_remove\n",
    "            next_board = board_copy\n",
    "            value = 0 if self.states_value.get(str(next_board)) is None else self.states_value.get(str(next_board))\n",
    "            if value >= value_max:\n",
    "                value_max = value\n",
    "                move = p\n",
    "        return move\n",
    "    def play_action(self, action):\n",
    "        \"\"\"play a given action.\"\"\"\n",
    "        pile_number, amount_to_remove=action\n",
    "        self.board[pile_number]-=amount_to_remove\n",
    "        return\n",
    "    def training_game(self, rounds=10):\n",
    "        \"\"\"The function for training the agent.\n",
    "            Input is the number of training games to be played\"\"\"\n",
    "        for i in range(rounds):\n",
    "            while not self.is_game_over():\n",
    "                if self.player=='dummy':\n",
    "                    move= self.choose_random_action()\n",
    "                else:\n",
    "                    if np.random.uniform(0,1)<= self.exp_rate:\n",
    "                        move = self.choose_random_action()\n",
    "                    else:\n",
    "                        move=self.choose_smart_action()\n",
    "                self.play_action(move)\n",
    "                if self.player=='agent':\n",
    "                    self.states.append(self.board.copy())\n",
    "                if self.is_game_over():\n",
    "                    self.winner=self.player\n",
    "                    list_of_winners.append(self.winner)\n",
    "                    if self.winner=='agent':\n",
    "                        self.feedReward(5)\n",
    "                    else:\n",
    "                        self.feedReward(-1)\n",
    "                    self.reset(initial_board.copy())\n",
    "                    break\n",
    "                else:\n",
    "                    self.change_player()\n",
    "    def train_against_pro(self, rounds=1000):\n",
    "        \"\"\"In this function, the agent is trained against a player who only performs optimal moves\n",
    "        - note that this player is still named dummy.\n",
    "            Input is the number of training games to be played.\"\"\"\n",
    "        for i in range(rounds):\n",
    "            while not self.is_game_over():\n",
    "                if self.player=='dummy':\n",
    "                    move= self.choose_optimal_action()\n",
    "                else:\n",
    "                    if np.random.uniform(0,1)<= self.exp_rate:\n",
    "                        move = self.choose_random_action()\n",
    "                    else:\n",
    "                        move=self.choose_smart_action()\n",
    "                self.play_action(move)\n",
    "                if self.player=='agent':\n",
    "                    self.states.append(self.board.copy())\n",
    "                    #print(self.board, self.states, 'hej')\n",
    "                if self.is_game_over():\n",
    "                    self.winner=self.player\n",
    "                    list_of_winners.append(self.winner)\n",
    "                    if self.winner=='agent':\n",
    "                        self.feedReward(100)\n",
    "                    else:\n",
    "                        self.feedReward(-1)\n",
    "                    self.reset(initial_board.copy())\n",
    "                    break\n",
    "                else:\n",
    "                    self.change_player()\n",
    "    def test_against_pro(self, rounds=1000):\n",
    "        \"\"\"A function for testing a given policy against a player performing optimal moves.\n",
    "            Input is the number of test games to be played.\"\"\"\n",
    "        for i in range(rounds):\n",
    "            while not self.is_game_over():\n",
    "                if self.player=='dummy':\n",
    "                    move= self.choose_optimal_action()\n",
    "                    #print('dummy chooses the following move', move)\n",
    "                else:\n",
    "                        move=self.choose_smart_action()\n",
    "                        #print('agent chooses the following move', move)\n",
    "                self.play_action(move)\n",
    "                #print('the board is now:', self.board, 'with nim sum:', self.nim_sum(self.board))\n",
    "                if self.is_game_over():\n",
    "                    self.winner=self.player\n",
    "                    list_of_winners.append(self.winner)\n",
    "                    self.reset(initial_board.copy())\n",
    "                    break\n",
    "                else:\n",
    "                    self.change_player()\n",
    "    def test_game(self, rounds=1000):\n",
    "        \"\"\"A function for testing a policy against a player with the random strategy.\n",
    "            Input is the number of test game to be played.\"\"\"\n",
    "        for i in range(rounds):\n",
    "            while not self.is_game_over():\n",
    "                if self.player=='dummy':\n",
    "                    move= self.choose_random_action()\n",
    "                else:\n",
    "                        move=self.choose_smart_action()\n",
    "                self.play_action(move)\n",
    "                if self.is_game_over():\n",
    "                    self.winner=self.player\n",
    "                    list_of_winners.append(self.winner)\n",
    "                    self.reset(initial_board.copy())\n",
    "                    break\n",
    "                else:\n",
    "                    self.change_player()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "07cf45c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nim=NimGame(initial_board)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1a9d4a",
   "metadata": {},
   "source": [
    "## Below is to find the mean number of training games and mean number of seconds to find an optimal policy, when training and testing against a player using the random strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e716377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "In average, it took 17150.0 training games and 1.539172852039337 seconds to find an optimal policy.\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations=20\n",
    "max_win_rate=0\n",
    "rounds_mean=[]\n",
    "time_mean=[]\n",
    "for i in range(number_of_iterations):\n",
    "    print(i)\n",
    "    list_of_winners=[]\n",
    "    policy={}\n",
    "    nim.states_value=policy\n",
    "    total_rounds=0\n",
    "    rounds=1000\n",
    "    start_time=time.time()\n",
    "    while list_of_winners.count('agent')*100/rounds<100:\n",
    "        list_of_winners=[]\n",
    "        nim.training_game(rounds)\n",
    "        list_of_winners=[]\n",
    "        nim.test_game(rounds)\n",
    "        total_rounds+=rounds\n",
    "        #print('agent wins:', list_of_winners.count('agent')*100/rounds, total_rounds)\n",
    "        if list_of_winners.count('agent')*100/rounds>max_win_rate:\n",
    "            max_win_rate=list_of_winners.count('agent')*100/rounds\n",
    "    end_time=time.time()\n",
    "    rounds_mean.append(total_rounds)\n",
    "    time_mean.append(end_time-start_time)\n",
    "    #print(total_rounds, end_time-start_time)\n",
    "print('In average, it took', np.mean(rounds_mean),'training games and', np.mean(time_mean), 'seconds to find an optimal policy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694a31a",
   "metadata": {},
   "source": [
    "### The cell below allows to test the last policy obtained in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "2b8b5330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent wins: 100.0 % of the games against a player with the random strategy\n",
      "agent wins: 100.0 % of the games against a player with the optimal strategy\n"
     ]
    }
   ],
   "source": [
    "list_of_winners=[]\n",
    "nim.test_game(rounds)\n",
    "print('agent wins:', list_of_winners.count('agent')*100/rounds, '% of the games against a player with the random strategy')\n",
    "list_of_winners=[]\n",
    "nim.test_against_pro(rounds)\n",
    "print('agent wins:', list_of_winners.count('agent')*100/rounds, '% of the games against a player with the optimal strategy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72908d3",
   "metadata": {},
   "source": [
    "## Below is to find the mean number of training games and mean number of seconds to find an optimal policy, when training against a player using the random strategy and testing against a player with the optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd702df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In average, it took 19050.0 training games and 2.182584536075592 seconds to find an optimal policy.\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations=20\n",
    "rounds_mean=[]\n",
    "time_mean=[]\n",
    "for i in range(number_of_iterations):\n",
    "    list_of_winners=[]\n",
    "    policy={}\n",
    "    nim.states_value=policy\n",
    "    total_rounds=0\n",
    "    rounds=1000\n",
    "    start_time=time.time()\n",
    "    while list_of_winners.count('agent')*100/rounds<100:\n",
    "        list_of_winners=[]\n",
    "        nim.training_game(rounds)\n",
    "        list_of_winners=[]\n",
    "        nim.test_against_pro(rounds)\n",
    "        total_rounds+=rounds\n",
    "        #print('agent wins:', list_of_winners.count('agent')*100/rounds, total_rounds)\n",
    "        if list_of_winners.count('agent')*100/rounds>max_win_rate:\n",
    "            max_win_rate=list_of_winners.count('agent')*100/rounds\n",
    "    end_time=time.time()\n",
    "    rounds_mean.append(total_rounds)\n",
    "    time_mean.append(end_time-start_time)\n",
    "    #print(total_rounds, end_time-start_time)\n",
    "print('In average, it took', np.mean(rounds_mean),'training games and', np.mean(time_mean), 'seconds to find an optimal policy.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6283ea40",
   "metadata": {},
   "source": [
    "## Below is to find the mean number of training games and mean number of seconds to find an optimal policy, when training and testing against a player using the optimal strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "e3abad3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In average, it took 1000.0 training games and 0.10912256240844727 seconds to find an optimal policy.\n"
     ]
    }
   ],
   "source": [
    "number_of_iterations=20\n",
    "rounds_mean=[]\n",
    "time_mean=[]\n",
    "for i in range(number_of_iterations):\n",
    "    list_of_winners=[]\n",
    "    policy={}\n",
    "    nim.states_value=policy\n",
    "    total_rounds=0\n",
    "    rounds=1000\n",
    "    start_time=time.time()\n",
    "    while list_of_winners.count('agent')*100/rounds<100:\n",
    "        list_of_winners=[]\n",
    "        nim.train_against_pro(rounds)\n",
    "        list_of_winners=[]\n",
    "        nim.test_against_pro(rounds)\n",
    "        total_rounds+=rounds\n",
    "        #print('agent wins:', list_of_winners.count('agent')*100/rounds, total_rounds)\n",
    "        if list_of_winners.count('agent')*100/rounds>max_win_rate:\n",
    "            max_win_rate=list_of_winners.count('agent')*100/rounds\n",
    "    end_time=time.time()\n",
    "    rounds_mean.append(total_rounds)\n",
    "    time_mean.append(end_time-start_time)\n",
    "    #print(total_rounds, end_time-start_time)\n",
    "print('In average, it took', np.mean(rounds_mean),'training games and', np.mean(time_mean), 'seconds to find an optimal policy.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "9cd1b8ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[3, 0]': 0.26382104116971833,\n",
       " '[1, 0]': 5.7162118005828155,\n",
       " '[1, 3]': -0.04360312565677414,\n",
       " '[3, 4]': -0.3989787047473254,\n",
       " '[3, 1]': -0.49607643868958107,\n",
       " '[1, 5]': -0.27153024578615564,\n",
       " '[0, 5]': 2.030846617443289,\n",
       " '[0, 1]': 2.1879985124615615,\n",
       " '[2, 1]': -0.2794442255918559,\n",
       " '[3, 3]': 11.403877974899833,\n",
       " '[0, 2]': -0.2571038250326191,\n",
       " '[2, 5]': -0.31277553712234285,\n",
       " '[1, 2]': -0.2104838570811754,\n",
       " '[3, 2]': -0.3572855407835115,\n",
       " '[2, 0]': -0.24407521440403018,\n",
       " '[0, 0]': 27.988329071742296,\n",
       " '[1, 1]': 9.43119351756959,\n",
       " '[2, 2]': 1.819429520442433,\n",
       " '[0, 3]': -0.07769159369677414,\n",
       " '[2, 3]': -0.036250837446204345}"
      ]
     },
     "execution_count": 872,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf0dd9d",
   "metadata": {},
   "source": [
    "## Below is to find the highest values boards in the policy found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "id": "61496035",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0] 392.01140300034746 0\n",
      "[1, 1] -18.70541873037432 0\n",
      "[2, 2] -33.994012623969816 0\n",
      "[2, 3] -34.19266400944704 1\n",
      "[1, 3] -34.673776287668886 2\n",
      "[0, 3] -36.37541362992991 3\n",
      "[1, 2] -59.61555056478125 3\n",
      "[0, 2] -60.65206542224638 2\n",
      "[2, 0] -61.560749291072895 2\n",
      "[2, 1] -62.479975528018926 3\n",
      "[3, 3] -88.15937832122076 0\n",
      "[0, 5] -88.42828081616597 5\n",
      "[3, 2] -88.48862702675608 1\n",
      "[3, 4] -89.49960824655686 7\n",
      "[2, 5] -89.6334055349269 7\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "my_dict=nim.states_value\n",
    "sorted_dict = sorted(my_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "top_keys = [x[0] for x in sorted_dict[:15]]\n",
    "\n",
    "best_boards=top_keys\n",
    "for i in best_boards:\n",
    "    print( i,my_dict[str(i)],nim_sum(ast.literal_eval(i)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
